<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Peter Bouda - AI</title><link href="https://www.peterbouda.eu/" rel="alternate"></link><link href="https://www.peterbouda.eu/feeds/ai.atom.xml" rel="self"></link><id>https://www.peterbouda.eu/</id><updated>2025-11-13T10:32:00+00:00</updated><subtitle>Natural Language Processing Consultant</subtitle><entry><title>New Workshop AI Technologies for Backend and Full-StackÂ Developers</title><link href="https://www.peterbouda.eu/new-workshop-ai-technologies-for-backend-and-full-stack-developers.html" rel="alternate"></link><published>2025-11-13T10:32:00+00:00</published><updated>2025-11-13T10:32:00+00:00</updated><author><name>Peter Bouda</name></author><id>tag:www.peterbouda.eu,2025-11-13:/new-workshop-ai-technologies-for-backend-and-full-stack-developers.html</id><summary type="html">&lt;p&gt;Just launched a new workshop: &amp;#8220;&lt;span class="caps"&gt;AI&lt;/span&gt; Technologies for Backend and Full-Stack&amp;nbsp;Developers&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Over the past months I noticed how many developers struggle to integrate &lt;span class="caps"&gt;AI&lt;/span&gt; capabilities into their applications. Not because &lt;span class="caps"&gt;AI&lt;/span&gt; is hard, but because the landscape is fragmented. You have frontend frameworks, backend services, APIs, caching strategies, rate â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Just launched a new workshop: &amp;#8220;&lt;span class="caps"&gt;AI&lt;/span&gt; Technologies for Backend and Full-Stack&amp;nbsp;Developers&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Over the past months I noticed how many developers struggle to integrate &lt;span class="caps"&gt;AI&lt;/span&gt; capabilities into their applications. Not because &lt;span class="caps"&gt;AI&lt;/span&gt; is hard, but because the landscape is fragmented. You have frontend frameworks, backend services, APIs, caching strategies, rate limiting, cost management â€“ and then you try to add LLMs on top of&amp;nbsp;everything.&lt;/p&gt;
&lt;p&gt;This workshop addresses that. Three full days (or six half days) where we work through practical integration patterns. How do you build conversational interfaces with streaming responses? How do you implement intelligent search without rebuilding your entire stack? What&amp;#8217;s the right caching strategy for &lt;span class="caps"&gt;LLM&lt;/span&gt;&amp;nbsp;responses?&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll work with modern frameworks across the stack â€“ Java Spring Boot, Go, React â€“ whatever fits your architecture. And beyond just integrating &lt;span class="caps"&gt;AI&lt;/span&gt; into applications, we&amp;#8217;ll explore how &lt;span class="caps"&gt;AI&lt;/span&gt; tools can accelerate your development process itself. From &lt;span class="caps"&gt;AI&lt;/span&gt;-assisted coding to automated testing and&amp;nbsp;documentation.&lt;/p&gt;
&lt;p&gt;The workshop is available online or in-house, in English or German, and adapted to your team&amp;#8217;s&amp;nbsp;needs.&lt;/p&gt;
&lt;p&gt;Full details: &lt;a href="https://www.aiworkshops.eu/pages/workshop-ai-technologies-for-backend-and-full-stack-developers.html"&gt;https://www.aiworkshops.eu/pages/workshop-ai-technologies-for-backend-and-full-stack-developers.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let me know if you have questions about the content or&amp;nbsp;format!&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category></entry><entry><title>Running AI atÂ home</title><link href="https://www.peterbouda.eu/running-ai-at-home.html" rel="alternate"></link><published>2025-10-15T16:47:00+01:00</published><updated>2025-10-15T16:47:00+01:00</updated><author><name>Peter Bouda</name></author><id>tag:www.peterbouda.eu,2025-10-15:/running-ai-at-home.html</id><summary type="html">&lt;p&gt;The first reviews of &lt;span class="caps"&gt;NVIDIA&lt;/span&gt; &lt;span class="caps"&gt;DGX&lt;/span&gt; Spark review are being published, so I want to
write about the current state of self-hosting&amp;nbsp;LLMs.&lt;/p&gt;
&lt;p&gt;First, I think it&amp;#8217;s quite amazing that you can run state-of-the-art models on
your own machines. This sets &lt;span class="caps"&gt;AI&lt;/span&gt; apart from technologies like search engines and â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;The first reviews of &lt;span class="caps"&gt;NVIDIA&lt;/span&gt; &lt;span class="caps"&gt;DGX&lt;/span&gt; Spark review are being published, so I want to
write about the current state of self-hosting&amp;nbsp;LLMs.&lt;/p&gt;
&lt;p&gt;First, I think it&amp;#8217;s quite amazing that you can run state-of-the-art models on
your own machines. This sets &lt;span class="caps"&gt;AI&lt;/span&gt; apart from technologies like search engines and
cloud service providing that are hard to replicate &amp;#8220;at home&amp;#8221;. It brings back
experiences from personal computing after the mainframe era:
&lt;a href="https://www.computinghistory.org.uk/pages/3971/There-is-no-reason-anyone-would-want-a-computer-in-their-home"&gt;&amp;#8220;There is no reason for any individual to have a computer in their home&amp;#8221;&lt;/a&gt;&amp;nbsp;:)&lt;/p&gt;
&lt;p&gt;Models that you can run at home nowadays show performance similar to models that
the big &lt;span class="caps"&gt;AI&lt;/span&gt; companies published 9-12 months ago, so &lt;span class="caps"&gt;GPT&lt;/span&gt;-4 performance is not
compeletely out of&amp;nbsp;reach.&lt;/p&gt;
&lt;p&gt;The main problem that home &lt;span class="caps"&gt;AI&lt;/span&gt; enthusiasts currently face is limited, fast
memory. The LLMs that reach &lt;span class="caps"&gt;GPT&lt;/span&gt;-4 performance require probably more than &lt;span class="caps"&gt;128GB&lt;/span&gt;
of memory, which might be out-of-scope with limited financial budgets. If you
want to run &lt;span class="caps"&gt;AI&lt;/span&gt; with reasonable resources you basically have two&amp;nbsp;options:&lt;/p&gt;
&lt;p&gt;First, you buy a consumer &lt;span class="caps"&gt;GPU&lt;/span&gt; where currently the maximum memory would be &lt;span class="caps"&gt;24GB&lt;/span&gt;.
But that memory is super-fast, which is what you&amp;nbsp;want.&lt;/p&gt;
&lt;p&gt;Second, you use the option with unified memory that is shared between &lt;span class="caps"&gt;CPU&lt;/span&gt; and
&lt;span class="caps"&gt;GPU&lt;/span&gt;, where you can choose among Apple&amp;#8217;s platforms, &lt;span class="caps"&gt;AMD&lt;/span&gt; Ryzen &lt;span class="caps"&gt;AI&lt;/span&gt; Max or now the
&lt;span class="caps"&gt;NVIDIA&lt;/span&gt; &lt;span class="caps"&gt;DGX&lt;/span&gt; Spark. Unfortunately unified memory is slower then pure &lt;span class="caps"&gt;GPU&lt;/span&gt; memory,
the fastest you get with Apple&amp;#8217;s M* Max series. But up to &lt;span class="caps"&gt;64GB&lt;/span&gt; memory or even
&lt;span class="caps"&gt;128GB&lt;/span&gt; are possible&amp;nbsp;then!&lt;/p&gt;
&lt;p&gt;When you know your memory size you can begin to choose the models. Normally
models come in a size labelled in billions, like &amp;#8220;30b&amp;#8221;, which means they have
so-and-so many paramers, e.g. 30 billion parameters. Each parameter in standard
models has 2 bytes, so a &amp;#8220;30b&amp;#8221; model is roughly 60 gigabytes. With unified
memory you need some memory to run applications besides the models, so a
standard 30b model will barely fit into &lt;span class="caps"&gt;64GB&lt;/span&gt; unified memory, and definitely not
on any consumer &lt;span class="caps"&gt;GPU&lt;/span&gt;. People started to &amp;#8220;quantize&amp;#8221; models, so that each parameter
has only 1 byte or even half a byte, and voilÃ , 30b models fit into &lt;span class="caps"&gt;24GB&lt;/span&gt; &lt;span class="caps"&gt;GPU&lt;/span&gt;
memory (maybe try &lt;a href="https://ollama.com/library/qwen3:30b-a3b-instruct-2507-q4_K_M"&gt;Qwen3 30b&lt;/a&gt;,
my standard model now for local inference) and a 106b model into your &lt;span class="caps"&gt;64GB&lt;/span&gt;
Macbook Pro (possibly
&lt;a href="https://simonwillison.net/2025/Jul/29/space-invaders/"&gt;&lt;span class="caps"&gt;GLM&lt;/span&gt;-4.5 Air&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;To run models you can start with &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope that helps to start your first local chat :) and let me know about any
doubts or questions about running your local &lt;span class="caps"&gt;AI&lt;/span&gt;!&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category></entry><entry><title>ðŸ”¬ Fascinating research from NVIDIA: Small Language Models are the Future of Agentic AI</title><link href="https://www.peterbouda.eu/fascinating-research-from-nvidia-small-language-models-are-the-future-of-agentic-ai.html" rel="alternate"></link><published>2025-09-30T10:36:00+01:00</published><updated>2025-09-30T10:36:00+01:00</updated><author><name>Peter Bouda</name></author><id>tag:www.peterbouda.eu,2025-09-30:/fascinating-research-from-nvidia-small-language-models-are-the-future-of-agentic-ai.html</id><summary type="html">&lt;p&gt;While everyone&amp;#8217;s focused on making models bigger, this paper makes a compelling case for going smallerâ€”especially for agentic &lt;span class="caps"&gt;AI&lt;/span&gt;&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;The core insight: Most &lt;span class="caps"&gt;AI&lt;/span&gt; agents don&amp;#8217;t need vast general knowledge. They perform specialized, repetitive tasks where fine-tuned small language models (SLMs) are not just sufficientâ€”they â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;While everyone&amp;#8217;s focused on making models bigger, this paper makes a compelling case for going smallerâ€”especially for agentic &lt;span class="caps"&gt;AI&lt;/span&gt;&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;The core insight: Most &lt;span class="caps"&gt;AI&lt;/span&gt; agents don&amp;#8217;t need vast general knowledge. They perform specialized, repetitive tasks where fine-tuned small language models (SLMs) are not just sufficientâ€”they&amp;#8217;re&amp;nbsp;superior.&lt;/p&gt;
&lt;p&gt;Why this matters for&amp;nbsp;Europe:&lt;/p&gt;
&lt;p&gt;âš¡ Energy efficiency at scale â€“ SLMs consume significantly less power than their large counterparts. As &lt;span class="caps"&gt;AI&lt;/span&gt; deployment grows, this shift could dramatically reduce the carbon footprint and operational costs of &lt;span class="caps"&gt;AI&lt;/span&gt; infrastructure across the &lt;span class="caps"&gt;EU&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;ðŸ‡ªðŸ‡º Sovereign technology advantage â€“ Smaller models are far more economical to train and deploy. This lowers barriers for European companies and research institutions to develop and control their own &lt;span class="caps"&gt;AI&lt;/span&gt; technologies, reducing dependence on hyperscale infrastructure and foreign tech&amp;nbsp;giants.&lt;/p&gt;
&lt;p&gt;ðŸŽ¯ Task-specific fine-tuning â€“ The paper demonstrates that SLMs fine-tuned for specific workflows can match or exceed the performance of general-purpose LLMs in agentic systems, while being orders of magnitude more&amp;nbsp;cost-effective.&lt;/p&gt;
&lt;p&gt;For Europe&amp;#8217;s &lt;span class="caps"&gt;AI&lt;/span&gt; strategyâ€”balancing innovation, sovereignty, and sustainabilityâ€”this approach could be&amp;nbsp;transformative.&lt;/p&gt;
&lt;p&gt;Full paper: &lt;a href="https://research.nvidia.com/labs/lpr/slm-agents/"&gt;https://research.nvidia.com/labs/lpr/slm-agents/&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category></entry><entry><title>Talk about multilingual agentÂ evaluation</title><link href="https://www.peterbouda.eu/talk-about-multilingual-agent-evaluation.html" rel="alternate"></link><published>2025-09-11T09:20:00+01:00</published><updated>2025-09-11T09:20:00+01:00</updated><author><name>Peter Bouda</name></author><id>tag:www.peterbouda.eu,2025-09-11:/talk-about-multilingual-agent-evaluation.html</id><summary type="html">&lt;p&gt;This Thursday, today, I am happy to give a talk about multilingual agents and their performance on European languages
&lt;a href="https://luma.com/bszaz8e3"&gt;at the &lt;span class="caps"&gt;AI&lt;/span&gt; Agent World Tour: Lisbon MLOps Meetup&lt;/a&gt;. We tested a few LLMs that you can run on
consumer GPUs to find out which work best for agentic use cases â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This Thursday, today, I am happy to give a talk about multilingual agents and their performance on European languages
&lt;a href="https://luma.com/bszaz8e3"&gt;at the &lt;span class="caps"&gt;AI&lt;/span&gt; Agent World Tour: Lisbon MLOps Meetup&lt;/a&gt;. We tested a few LLMs that you can run on
consumer GPUs to find out which work best for agentic use cases. I hope to see you&amp;nbsp;there!&lt;/p&gt;
&lt;p&gt;We just released the repository so you can find all code, results and visualizations&amp;nbsp;here:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Nodehaus/evaluation/tree/main/multilingual_agent"&gt;https://github.com/Nodehaus/evaluation/tree/main/multilingual_agent&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category></entry><entry><title>The Great European AI LanguageÂ Championship</title><link href="https://www.peterbouda.eu/the-great-european-ai-language-championship.html" rel="alternate"></link><published>2025-09-01T14:42:00+01:00</published><updated>2025-09-01T14:42:00+01:00</updated><author><name>Peter Bouda</name></author><id>tag:www.peterbouda.eu,2025-09-01:/the-great-european-ai-language-championship.html</id><summary type="html">&lt;p&gt;We published &lt;a href="https://substack.com/home/post/p-172471752"&gt;an article today about our multilingual evaluation of smaller LLMs&lt;/a&gt; using the belebele dataset.
&lt;a href="https://github.com/facebookresearch/belebele"&gt;Belebele&lt;/a&gt; is an easy-to-use dataset with multiple choice question in 122 langauge variants, we focussed on 8 European languages.
The gemma-3 models performed best, but open models like OLLMo-2 showed quite good performance, too â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;We published &lt;a href="https://substack.com/home/post/p-172471752"&gt;an article today about our multilingual evaluation of smaller LLMs&lt;/a&gt; using the belebele dataset.
&lt;a href="https://github.com/facebookresearch/belebele"&gt;Belebele&lt;/a&gt; is an easy-to-use dataset with multiple choice question in 122 langauge variants, we focussed on 8 European languages.
The gemma-3 models performed best, but open models like OLLMo-2 showed quite good performance, too! Read more in the full&amp;nbsp;article:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://substack.com/home/post/p-172471752"&gt;https://substack.com/home/post/p-172471752&lt;/a&gt;&lt;/p&gt;</content><category term="AI"></category><category term="AI"></category></entry><entry><title>Running tinygrad with CUDA onÂ NixOS</title><link href="https://www.peterbouda.eu/running-tinygrad-with-cuda-on-nixos.html" rel="alternate"></link><published>2025-06-16T08:40:00+01:00</published><updated>2025-06-16T08:40:00+01:00</updated><author><name>Peter Bouda</name></author><id>tag:www.peterbouda.eu,2025-06-16:/running-tinygrad-with-cuda-on-nixos.html</id><summary type="html">&lt;p&gt;I started to play around with NixOS a few weeks ago and until now I am quite happy with it. I installed it on an older
&lt;span class="caps"&gt;PC&lt;/span&gt; with a Geforce &lt;span class="caps"&gt;GT&lt;/span&gt; 1030 and wanted to see if I get tinygrad running with the &lt;span class="caps"&gt;CUDA&lt;/span&gt; back-end on it. I learned about â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I started to play around with NixOS a few weeks ago and until now I am quite happy with it. I installed it on an older
&lt;span class="caps"&gt;PC&lt;/span&gt; with a Geforce &lt;span class="caps"&gt;GT&lt;/span&gt; 1030 and wanted to see if I get tinygrad running with the &lt;span class="caps"&gt;CUDA&lt;/span&gt; back-end on it. I learned about
nix-shell and, after a while, was able to set up such a shell that supports tinygrad on &lt;span class="caps"&gt;CUDA&lt;/span&gt;. I was not able to find
examples that worked directly for my use case, so I will post that here, for future reference. The main customizations
for me&amp;nbsp;were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding clang as the shell&amp;nbsp;compiler&lt;/li&gt;
&lt;li&gt;LD_LIBRARY_PATH&amp;nbsp;needs &lt;code&gt;${pkgs.cudatoolkit}/lib&lt;/code&gt; for &lt;code&gt;libnvrtc.so&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;I&amp;nbsp;added &lt;code&gt;NIX_ENFORCE_NO_NATIVE=0&lt;/code&gt; so that tinygrad can&amp;nbsp;pass &lt;code&gt;-march=native&lt;/code&gt; to&amp;nbsp;clang&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is my complete&amp;nbsp;nix-shell:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;nixpkgs&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{};&lt;/span&gt;
&lt;span class="n"&gt;clangStdenv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mkDerivation&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;clang-nix-shell&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# nativeBuildInputs is usually what you want -- tools you need to run&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;nativeBuildInputs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pkgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;buildPackages&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;python312&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;uv&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;buildInputs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pkgs&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gitRepo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gnupg&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;autoconf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;curl&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;procps&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gnumake&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;util&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;linux&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;m4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gperf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;unzip&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;cudatoolkit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;linuxPackages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nvidia_x11&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;libGLU&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;libGL&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;xorg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;libXi&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;xorg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;libXmu&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;freeglut&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;xorg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;libXext&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;xorg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;libX11&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;xorg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;libXv&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;xorg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;libXrandr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;zlib&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;ncurses5&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stdenv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cc&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;binutils&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;shellHook&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;CUDA_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;pkgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cudatoolkit&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;pkgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linuxPackages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nvidia_x11&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;pkgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ncurses5&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;pkgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cudatoolkit&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;EXTRA_LDFLAGS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;-L/lib -L$&lt;/span&gt;&lt;span class="si"&gt;{pkgs.linuxPackages.nvidia_x11}&lt;/span&gt;&lt;span class="s2"&gt;/lib&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;EXTRA_CCFLAGS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;-I/usr/include&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;NIX_ENFORCE_NO_NATIVE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You might also need to remove&amp;nbsp;the &lt;code&gt;--target&lt;/code&gt; argument to clang from the&amp;nbsp;tinygrad &lt;code&gt;runtime/ops_cpu.py&lt;/code&gt;. It gave me
warning, but I am not sure if it actually &lt;a href="https://github.com/NixOS/nixpkgs/pull/323869"&gt;breaks things&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="mf"&gt;14&lt;/span&gt;&lt;span class="n"&gt;c14&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;[&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;march&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;native&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;none&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;unknown&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;elf&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;O2&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fPIC&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ffreestanding&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kd"&gt;fn&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;errno&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;nostdlib&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kd"&gt;fn&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ident&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;]&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;[&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;march&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;native&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;O2&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fPIC&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ffreestanding&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kd"&gt;fn&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;errno&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;nostdlib&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kd"&gt;fn&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ident&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="AI"></category><category term="AI"></category></entry></feed>